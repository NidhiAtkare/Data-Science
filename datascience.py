# -*- coding: utf-8 -*-
"""DS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XMNKsGKKzSYrvOH_BGaOl0QX1uvj1qN5

TASK 2: Data Loading & Cleaning and Data Preprocessing:
 * Data cleaning involves loading and inspecting data, removing nulls & duplicates, fixing data types, renaming columns, checking outliers, and standardizing formats.
"""

# Step 1: Data Loading & Cleaning

import pandas as pd
import numpy as np
import streamlit as st

# Load dataset
file_path = "NCRB_CII-2019_Table_17A.1 (1).csv"
df = pd.read_csv(file_path, low_memory=False)

print("‚úÖ File Loaded Successfully")
print("Shape of dataset:", df.shape)

# Preview data
print("\nüîπ First 5 rows:")
print(df.head())

# Check basic info
print("\nüîπ Dataset Info:")
print(df.info())

# Remove duplicate rows
df = df.drop_duplicates()
print("\n‚úÖ Duplicates removed. New shape:", df.shape)

# Check missing values
print("\nüîπ Missing values per column:")
print(df.isnull().sum())

# Fill missing numeric values with 0
num_cols = df.select_dtypes(include=[np.number]).columns
df[num_cols] = df[num_cols].fillna(0)

# Fill missing string values with "Unknown"
str_cols = df.select_dtypes(include=["object"]).columns
df[str_cols] = df[str_cols].fillna("Unknown")

print("\n‚úÖ Missing values handled.")

# Fix numeric columns that contain commas (e.g., "1,234")
for col in df.columns:
    if df[col].dtype == "object":
        df[col] = df[col].str.replace(',', '', regex=True)
        # Convert to numeric if possible
        df[col] = pd.to_numeric(df[col], errors='ignore')

print("\n‚úÖ Data type conversion completed.")

# Rename columns with special characters for easier plotting
df = df.rename(columns={
    'Cases Pending Investigation from Previous Year': 'Cases Pending Investigation (Prev Year)',
    'Total Cases for Investigation (Col.3+ Col.4+ Col.5)': 'Total Cases for Investigation',
    'Final Report - Total (Col.10+ Col.11+ Col.12+ Col.13+ Col.14)': 'Final Report - Total',
    'Chargesheets submitted - Cases Chargesheeted Out of Cases from Previous Year': 'Chargesheets submitted - Cases Chargesheeted (Prev Year)',
    'Chargesheets submitted - Cases Chargesheeted Out of Cases during the Year': 'Chargesheets submitted - Cases Chargesheeted (Current Year)',
    'Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)': 'Chargesheets submitted - Cases Chargesheeted',
    'Total Cases Disposed Off by Police (Col.7+ Col.8+ Col.15+ Col.18)': 'Total Cases Disposed Off by Police',
    'Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)': 'Cases Pending Investigation at End of Year',
    'Charge-Sheeting Rate (Col.18/ Col.19) *100': 'Charge-Sheeting Rate (%)',
    'Pendency Percentage (Col.22/ Col.6) *100': 'Pendency Percentage (%)'
})


# Show cleaned dataset preview
print("\nüîπ Cleaned Dataset Preview:")
print(df.head())

# Save cleaned file
df.to_csv("cleaned_crime_data.csv", index=False)
print("\nüíæ Cleaned data saved as: cleaned_crime_data.csv")

# 1. Display the first 5 rows to see what the data looks like
print("First 5 rows of the dataset:")
st.dataframe(df.head())

# 2. Get a concise summary of the dataframe
print("\nDataset Information:")
st.text(df.info(buf=None))

# 3. Get descriptive statistics for numerical columns
print("\nDescriptive Statistics:")
st.dataframe(df.describe())

# 4. Check the dimensions of the dataframe (rows, columns)
print(f"\nDataset Shape: {df.shape}")

"""What the Previous Code Did:
  * Loaded the dataset using pandas

 * Displayed first few rows to inspect data

 * Checked data summary and column types

 * Checked for missing values

 * Removed rows with null values

 * Removed duplicate rows

 * \Converted text-based numeric columns into numeric format

TASK 3: Visualize Data Distribution:
"""

# Create a figure to hold the four plots
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 10))
plt.suptitle('Distribution of Crime Data Metrics', fontsize=16)

# Plot 1: Distribution of Cases Reported
plt.subplot(2, 2, 1)
sns.histplot(df['Cases Reported during the year'], kde=True, bins=30, color='skyblue')
plt.title('Distribution of Cases Reported')
plt.xlabel('Cases Reported')
plt.ylabel('Frequency')

# Plot 2: Distribution of Cases Chargesheeted
plt.subplot(2, 2, 2)
sns.histplot(df['Chargesheets submitted - Cases Chargesheeted'], kde=True, bins=30, color='lightcoral')
plt.title('Distribution of Cases Chargesheeted')
plt.xlabel('Cases Chargesheeted')
plt.ylabel('Frequency')

# Plot 3: Distribution of Charge Sheeting Rate
plt.subplot(2, 2, 3)
sns.histplot(df['Charge-Sheeting Rate (%)'], kde=True, bins=30, color='lightgreen')
plt.title('Distribution of Charge Sheeting Rate (%)')
plt.xlabel('Charge Sheeting Rate (%)')
plt.ylabel('Frequency')

# Plot 4: Distribution of Pendency Percentage
plt.subplot(2, 2, 4)
sns.histplot(df['Pendency Percentage (%)'], kde=True, bins=30, color='gold')
plt.title('Distribution of Pendency Percentage (%)')
plt.xlabel('Pendency Percentage (%)')
plt.ylabel('Frequency')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""Observations from Distribution Plots ‚Äî Crime Dataset:

 * Low Crime Counts are Most Common:
Most regions/states report lower numbers of crime cases, indicating crime is concentrated at a smaller scale in many areas.

 * High-Crime Outliers Exist:
A few states show very high crime counts, suggesting urban or high-population regions significantly influence totals.

 * Skewed Distribution:
Crime data is not uniform ‚Äî values are clustered near the lower range, with a long tail toward higher crime numbers.

 * Similar Trend Across Measures:
The distribution pattern for cases reported and cases chargesheeted looks similar, meaning areas with high reporting also tend to have high chargesheeting.

 * Charge-Sheeting Rate Variation:
While many states show moderate charge-sheeting rates, a few have very high or very low rates, indicating efficiency differences in completing investigations.
"""

# Select only numeric columns, excluding the rate and percentage columns
numeric_df = df.select_dtypes(include=['int64', 'float64']).drop(columns=['Charge-Sheeting Rate (%)', 'Pendency Percentage (%)'])

# Compute correlation matrix
corr = numeric_df.corr()

# Plot heatmap using seaborn
plt.figure(figsize=(12, 10)) # Increased figure size for better readability
sns.heatmap(corr, cmap='coolwarm', linewidths=.5)
plt.title("Correlation Heatmap of Crime Data")
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# Group data by State and sum total cases
state_crime = df.groupby('Crime Head')['Cases Reported during the year'].sum().sort_values(ascending=False)

# Plot bar chart
plt.figure(figsize=(20, 8)) # Increased figure size for better readability
plt.bar(state_crime.index, state_crime.values)
plt.title("Crime Cases by Crime Head")
plt.xlabel("Crime Head")
plt.ylabel("Total Crime Cases")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Create a pair plot for the numerical columns
numerical_cols = ['Cases Reported during the year', 'Chargesheets submitted - Cases Chargesheeted',
                  'Charge-Sheeting Rate (%)', 'Pendency Percentage (%)']

sns.pairplot(df[numerical_cols])
plt.suptitle('Pair Plot of Selected Crime Data Metrics', y=1.02) # Adjust title position
plt.show()

# -------------------------------------------------------
# üìä Top 10 Crime Categories
# -------------------------------------------------------

import matplotlib.pyplot as plt
import seaborn as sns

cases_col = 'Cases Reported during the year'

top10 = df[["Crime Head", cases_col]].sort_values(by=cases_col, ascending=False).head(10)

plt.figure(figsize=(10,6))
plt.barh(top10["Crime Head"], top10[cases_col])
plt.xlabel("Cases Reported")
plt.title("Top 10 Crime Categories in India (2019)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()


# -------------------------------------------------------
# ü•ß Pie Chart ‚Äî Crime Composition
# -------------------------------------------------------

others = df[cases_col].sum() - top10[cases_col].sum()
pie_data = top10.copy()
pie_data.loc[len(pie_data)] = ["Others", others]

plt.figure(figsize=(8,8))
plt.pie(pie_data[cases_col], labels=pie_data["Crime Head"], autopct="%1.1f%%")
plt.title("Crime Share Distribution ‚Äî India 2019")
plt.show()


# -------------------------------------------------------
# üìà Investigation Status Bar Chart
# -------------------------------------------------------

status_cols = [
    "Cases Pending Investigation (Prev Year)",
    "Cases Reported during the year",
    # "Cases Closed without investigation", # This column does not exist in the dataframe
]

status_df = df[status_cols].sum()

plt.figure(figsize=(10,5))
plt.bar(status_df.index, status_df.values)
plt.title("Investigation Status ‚Äî India 2019")
plt.ylabel("Number of Cases")
plt.xticks(rotation=45)
plt.show()


# -------------------------------------------------------
# ‚úÖ Summary Output
# -------------------------------------------------------

print("\n‚úÖ Analysis Completed ‚Äî Charts Generated Successfully\n")

"""TASK 4: Feature Engineering
 * Feature Engineering is the process of transforming raw data into meaningful inputs that improve a machine learning model‚Äôs performance.
It includes creating new features, modifying existing ones, encoding categorical values, handling missing data, and scaling numerical features so the model can learn patterns more effectively.
"""

from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import numpy as np

# 1. Handle missing values (already done in previous steps, but keeping for completeness)
df.fillna(0, inplace=True)

# 2. Create New Features

# Identify numeric crime columns, excluding 'S. No' and 'Crime Head' and the rate/percentage columns
numeric_crime_columns = df.select_dtypes(include=[np.number]).columns.tolist()
columns_to_exclude = ['S. No', 'Crime Head', 'Charge-Sheeting Rate (Col.18/ Col.19) *100', 'Pendency Percentage (Col.22/ Col.6) *100']
crime_columns_for_sum = [col for col in numeric_crime_columns if col not in columns_to_exclude]

# Total crime count
df['Total_Cases'] = df[crime_columns_for_sum].sum(axis=1)

# Crime per lakh population - Cannot calculate as Population column is not available
# df['Crime_Per_Lakh'] = (df['Total_Cases'] / df['Population']) * 100000

# Violent Crime Index (example crime col names‚Äîadjust based on your dataset)
# Using columns that appear to be related to violent crime based on dataset preview
violent_cols = ['Cases Reported during the year'] # This is a placeholder, adjust with actual violent crime columns if available
df['Violent_Crime_Index'] = df[violent_cols].sum(axis=1)

# Property Crime Index
# Using columns that appear to be related to property crime based on dataset preview
property_cols = ['Cases Reported during the year'] # This is a placeholder, adjust with actual property crime columns if available
df['Property_Crime_Index'] = df[property_cols].sum(axis=1)


# 3. Label Encode 'Crime Head' as 'State_Code' was intended for State/UT
label = LabelEncoder()
df['Crime_Head_Encoded'] = label.fit_transform(df['Crime Head'])


# 4. Scale numeric features
scale_cols = ['Total_Cases', 'Violent_Crime_Index', 'Property_Crime_Index']
scaler = MinMaxScaler()
df[scale_cols] = scaler.fit_transform(df[scale_cols])

st.dataframe(df.head())

"""Feature Engineering Steps Performed:

 * Selected important crime columns

 * Picked columns relevant for crime analysis (State, Year, Cases, etc.)

 * Created New Features

 * Added Total Crimes column by summing different crime types

 * Created additional ratio-based or combined indicators (like crime per state)

 * Handled Categorical Variables

 * Converted State/UT into numerical form using Label Encoding

 * Added Time-Based Feature

 * Treated Year as a numeric timeline for predictive modeling

 * Prepared Dataset for ML

 * Separated features (X) and target (y)

 * Converted Data Types

 * Ensured numeric columns are converted to numeric format if needed

TASK 5: Regression Model:
"""

# Features (X) and Target (y)
X = df[['Crime_Head_Encoded', 'Total_Cases']]
y = df['Cases Reported during the year']

# Train-Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression Model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation Metrics
from sklearn.metrics import mean_absolute_error, r2_score
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("üìä Regression Results:")
print("Mean Absolute Error (MAE):", mae)
print("R¬≤ Score:", r2)

import matplotlib.pyplot as plt
import pandas as pd # Ensure pandas is imported if not already in the context

# Filter data for one Crime Head (example: Murder)
crime_head_name = "Murder"

# Load the cleaned data specifically for this plot to ensure 'Crime Head' is a string
# This uses the 'cleaned_crime_data.csv' saved earlier, which has string 'Crime Head' values.
temp_df = pd.read_csv("cleaned_crime_data.csv")

# Rename columns to match the names used throughout the notebook for consistency
temp_df = temp_df.rename(columns={
    'Cases Pending Investigation from Previous Year': 'Cases Pending Investigation (Prev Year)',
    'Total Cases for Investigation (Col.3+ Col.4+ Col.5)': 'Total Cases for Investigation',
    'Final Report - Total (Col.10+ Col.11+ Col.12+ Col.13+ Col.14)': 'Final Report - Total',
    'Chargesheets submitted - Cases Chargesheeted Out of Cases from Previous Year': 'Chargesheets submitted - Cases Chargesheeted (Prev Year)',
    'Chargesheets submitted - Cases Chargesheeted Out of Cases during the Year': 'Chargesheets submitted - Cases Chargesheeted (Current Year)',
    'Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)': 'Chargesheets submitted - Cases Chargesheeted',
    'Total Cases Disposed Off by Police (Col.7+ Col.8+ Col.15+ Col.18)': 'Total Cases Disposed Off by Police',
    'Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)': 'Cases Pending Investigation at End of Year',
    'Charge-Sheeting Rate (Col.18/ Col.19) *100': 'Charge-Sheeting Rate (%)',
    'Pendency Percentage (Col.22/ Col.6) *100': 'Pendency Percentage (%)'
})

crime_head_df = temp_df[temp_df['Crime Head'] == crime_head_name]

# Select relevant columns for plotting
plot_data = crime_head_df[['Cases Reported during the year', 'Total Cases for Investigation']]

# Plot bar chart for the selected crime head
plt.figure(figsize=(8, 5))
plot_data.plot(kind='bar')
plt.title(f"Crime Cases for {crime_head_name}")
plt.xlabel("Metric")
plt.ylabel("Number of Cases")
plt.xticks(rotation=0) # Keep labels horizontal
plt.show()
plt.close() # Close the plot to manage memory

"""* Regression Model:Built a linear regression model using 'Crime_Head_Encoded' and 'Total_Cases' to predict 'Cases Reported during the year', and evaluated the model's performance.
 * Visualization for a Specific Crime Head:Created a bar plot to compare reported cases and total cases for investigation for a specific crime head (Murder).

TASK 6: Multi Linear Regression:
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("NCRB_CII-2019_Table_17A.1 (1).csv")

# Display basic info
print("First 5 rows of data:")
print(df.head())

# Select numeric columns only
numeric_cols = df.select_dtypes(include=np.number).columns.tolist()

# ‚úÖ Change this to your target column name
target_col = numeric_cols[-1]     # assuming last numeric column is target
X = df[numeric_cols[:-1]]         # all other numeric columns as features
y = df[target_col]

# Split data into train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluation
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print("\nModel Performance:")
print(f"R¬≤ Score: {r2:.4f}")
print(f"Mean Squared Error: {mse:.4f}")
print(f"Root Mean Squared Error: {rmse:.4f}")

# Coefficients
print("\nModel Coefficients:")
for feature, coef in zip(X.columns, model.coef_):
    print(f"{feature}: {coef:.4f}")
print(f"Intercept: {model.intercept_:.4f}")

# Plot actual vs predicted
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2)
plt.show()

"""TASK 7: Classification Modelling:"""

# Create Crime Category (Target Variable)
median_crime = df['Cases Reported during the year'].median()
df['Crime_Level'] = (df['Cases Reported during the year'] > median_crime).astype(int)

# Features (using Time_Index if state/year not available)
df = df.reset_index(drop=True)
df['Time_Index'] = df.index + 1  # treat rows as timeline

X = df[['Time_Index']]
y = df['Crime_Level']

# Train-Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression Model
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression()
clf.fit(X_train, y_train)

# Predictions
y_pred = clf.predict(X_test)

# Evaluation Metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""Converted crime counts into two categories ‚Äî High Crime and Low Crime ‚Äî using median thresholding.
Then, we trained a Logistic Regression model to classify crime levels.
Evaluation was done using accuracy, confusion matrix and classification report.

TASK 8: Applying K-Means:
"""

# Aggregate crime data by Crime Head
crime_agg = df.groupby('Crime Head')[[
    'Cases Pending Investigation from Previous Year',
    'Cases Reported during the year',
    'Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)',
    'Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)'
]].sum().reset_index()

print("Aggregated Crime Data by Crime Head:")
st.dataframe(crime_agg.head())

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Select numerical features for clustering
clustering_features = df[[
    'Cases Pending Investigation from Previous Year',
    'Cases Reported during the year',
    'Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)',
    'Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)'
]]

# Scale the features
scaler = StandardScaler()
scaled_clustering_features = scaler.fit_transform(clustering_features)

# Determine optimal number of clusters (Elbow Method)
inertia = []
k_range = range(1, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(scaled_clustering_features)
    inertia.append(kmeans.inertia_)

# Plot the Elbow Method
plt.figure(figsize=(8, 5))
plt.plot(k_range, inertia, marker='o')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.xticks(k_range)
plt.show()

"""* Selects numeric crime data columns for clustering (pending, reported,

chargesheeted, etc.)

 * Standardizes (scales) the data so all features contribute equally

 * Uses K-Means algorithm to form clusters of similar crime patterns

 * Runs K-Means for k = 1 to 10 clusters

 * Calculates inertia (cluster compactness) for each k

 * Lower inertia = better cluster grouping

 * Stores inertia values to compare cluster performance

 * Plots the Elbow Curve (inertia vs number of clusters)

 * Helps identify the optimal number of clusters by finding the curve ‚Äúbend‚Äù

 * This bend = best trade-off between few clusters and tight grouping

 * Prepares data to group crime categories based on investigation & disposal behavior

 * Enables detection of patterns & similarities in different crime types
"""

from sklearn.cluster import KMeans

# 4. Apply K-Means Clustering (on scaled dataset features)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters = kmeans.fit_predict(scaled_clustering_features)   # Use the correct variable name

# 5. Add Cluster Labels to Your Dataset
df['cluster'] = clusters   # df = your main dataset variable

print("Dataset with cluster labels added:")
st.dataframe(df.head())

plt.figure(figsize=(10, 7))
sns.scatterplot(
    data=df,
    x='Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)',
    y='Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)',
    hue='cluster',
    s=120
)
plt.title('K-Means Clustering ‚Äî Chargesheeted vs Pending at End')
plt.xlabel('Cases Chargesheeted')
plt.ylabel('Cases Pending at End of Year')
plt.legend(title='Cluster')
plt.grid(True)
plt.show()

"""üìä Interpretation of K-Means Clustering on Crime Investigation Metrics (k = 3)

Based on the scatter plots created using crime investigation features, the K-Means algorithm has grouped the observations into three distinct clusters.

üîπ Three Distinct Crime Pattern Clusters

The visualization shows that crime data points cluster into three separate groups, indicating different investigation and case-handling patterns across records.

* Cluster 0 ‚Äî High Crime Load / High Pending Cases

 * This cluster represents areas with:

 * Very high number of cases pending from previous year

 * High number of new cases reported

 * Higher number of cases pending at the end of the year

 * Comparatively higher investigation burden

Interpretation:
These may be high-population or high-crime regions where law enforcement faces a heavy investigation workload and cases accumulate over time.

* Cluster 1 ‚Äî Low Crime Volume / Efficient Resolution

 * This cluster includes most areas, characterized by:

 * Low number of cases pending

 * Fewer new cases reported

 * Lower pending cases at year end

 * Relatively balanced chargesheet submission

Interpretation:
These may be low-crime or better-managed regions with stable workload and effective investigation performance.

* Cluster 2 ‚Äî Moderate Crime, Higher Chargesheet Efficiency

 * This cluster includes regions with:

 * Moderate crime levels

 * Moderate pending cases

 * Higher number of cases chargesheeted compared to pending

 * Better case disposal performance

Interpretation:
These may be mid-crime regions showing good investigation efficiency‚Äîable to process many cases despite workload.

TASK 9: DBSCAN (Advanced k-means)
"""

# Identify numerical columns for clustering
numerical_quantity_cols = [
    'Cases Pending Investigation from Previous Year',
    'Cases Reported during the year',
    'Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)',
    'Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)'
]

# Create a new DataFrame with only these columns
crime_clustering_features = df[numerical_quantity_cols]

print("Features selected for crime clustering:")
st.dataframe(crime_clustering_features.head())

from sklearn.preprocessing import StandardScaler

# Instantiate a StandardScaler object
scaler = StandardScaler()

# Apply the scaler to the crime_clustering_features DataFrame
scaled_crime_clustering_features = scaler.fit_transform(crime_clustering_features)

print("Scaled crime clustering features (first 5 rows):")
scaled_crime_clustering_features[:5]

from sklearn.cluster import DBSCAN

# Instantiate a DBSCAN object
# eps & min_samples may need tuning after reviewing results
dbscan = DBSCAN(eps=0.5, min_samples=5)

# Fit DBSCAN model to the scaled crime features
dbscan.fit(scaled_crime_clustering_features)

# Store cluster labels
crime_cluster_labels = dbscan.labels_

print("DBSCAN clustering complete. Cluster labels generated.")
print("Number of clusters found (excluding noise):", len(set(crime_cluster_labels)) - (1 if -1 in crime_cluster_labels else 0))
print("Number of noise points:", list(crime_cluster_labels).count(-1))

# Add the cluster labels to the crime clustering features DataFrame
crime_clustering_features['cluster'] = crime_cluster_labels

print("Crime clustering features with cluster labels:")
st.dataframe(crime_clustering_features.head())

# Optionally add cluster labels to the main dataframe
# df['crime_cluster'] = crime_cluster_labels
# print("\nOriginal dataframe with crime cluster labels:")
# display(df.head())

# Visualize the clusters using scatter plots of crime feature pairs
plt.figure(figsize=(15, 10))
plt.suptitle('DBSCAN Clustering of Crime Data (eps=0.5, min_samples=5)', fontsize=16)

# Plot 1: Previous Year Pending vs Cases Reported
plt.subplot(2, 2, 1)
sns.scatterplot(
    x='Cases Pending Investigation from Previous Year',
    y='Cases Reported during the year',
    hue='cluster',
    data=crime_clustering_features,
    palette='viridis',
    s=50, alpha=0.6
)
plt.title('Pending (Prev Year) vs Cases Reported')
plt.xlabel('Cases Pending from Previous Year')
plt.ylabel('Cases Reported During Year')
plt.legend(title='Cluster')

# Plot 2: Chargesheeted vs Cases Pending End of Year
plt.subplot(2, 2, 2)
sns.scatterplot(
    x='Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)',
    y='Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)',
    hue='cluster',
    data=crime_clustering_features,
    palette='viridis',
    s=50, alpha=0.6
)
plt.title('Chargesheeted vs Pending (End Year)')
plt.xlabel('Cases Chargesheeted')
plt.ylabel('Pending Cases End Year')
plt.legend(title='Cluster')

# Plot 3: Previous Pending vs Chargesheeted
plt.subplot(2, 2, 3)
sns.scatterplot(
    x='Cases Pending Investigation from Previous Year',
    y='Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)',
    hue='cluster',
    data=crime_clustering_features,
    palette='viridis',
    s=50, alpha=0.6
)
plt.title('Previous Pending vs Chargesheeted')
plt.xlabel('Cases Pending from Previous Year')
plt.ylabel('Cases Chargesheeted')
plt.legend(title='Cluster')

# Plot 4: Cases Reported vs Pending End Year
plt.subplot(2, 2, 4)
sns.scatterplot(
    x='Cases Reported during the year',
    y='Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)',
    hue='cluster',
    data=crime_clustering_features,
    palette='viridis',
    s=50, alpha=0.6
)
plt.title('Cases Reported vs Pending (End Year)')
plt.xlabel('Cases Reported During Year')
plt.ylabel('Pending Cases End Year')
plt.legend(title='Cluster')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

df['cluster'] = crime_cluster_labels
st.dataframe(df.head())

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Find unique clusters (ignore noise = -1)
unique_clusters = set(df['cluster'])
if -1 in unique_clusters:
    unique_clusters.remove(-1)
num_clusters = len(unique_clusters)

print(f"Number of unique clusters found (excluding noise): {num_clusters}")

# 2. Count points per cluster
cluster_counts = df['cluster'].value_counts().sort_index()
print("\nNumber of data points per cluster:")
print(cluster_counts)

# 3. Automatically select numerical columns for mean calculation (crime features)
# Select a specific subset of relevant numerical features for visualization
# Fix: Use original column names as 'df' was reloaded with original names in 0XeGR9Jnwhvc
crime_features_for_plotting = [
    'Cases Pending Investigation from Previous Year',
    'Cases Reported during the year',
    'Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)',
    'Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)'
]

# Shorten the names of the features for plotting
# Fix: Update mapping to use original column names as keys
shortened_crime_features = {
    'Cases Pending Investigation from Previous Year': 'Pending (Prev Year)',
    'Cases Reported during the year': 'Reported (Current Year)',
    'Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)': 'Chargesheeted',
    'Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)': 'Pending (End Year)'
}

print("\nMean feature values per cluster (excluding noise):")
for cluster_label in sorted(unique_clusters):
    cluster_data = df[df['cluster'] == cluster_label]
    # Calculate mean of the selected features for each cluster
    mean_features = cluster_data[crime_features_for_plotting].mean()
    print(f"\nCluster {cluster_label}:")
    print(mean_features)

# 4. Visualize DBSCAN clusters using scatter plots of selected crime feature pairs
plt.figure(figsize=(15, 10))
plt.suptitle('DBSCAN Clustering ‚Äî Crime Dataset', fontsize=16)

plot_index = 1
for i in range(len(crime_features_for_plotting)):
    for j in range(i + 1, len(crime_features_for_plotting)):
        plt.subplot(2, 3, plot_index)
        sns.scatterplot(
            x=crime_features_for_plotting[i],
            y=crime_features_for_plotting[j],
            hue='cluster',
            data=df,
            s=30,
            alpha=0.7,
            palette='viridis'
        )
        plt.title(f"{shortened_crime_features[crime_features_for_plotting[i]]} vs {shortened_crime_features[crime_features_for_plotting[j]]}")
        plt.xlabel(shortened_crime_features[crime_features_for_plotting[i]])
        plt.ylabel(shortened_crime_features[crime_features_for_plotting[j]])
        plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')
        plot_index += 1

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""‚úÖ Interpretation of Clusters based on Mean Crime Feature Values

Features considered:

 * Pending (Prev Year) = Cases carried over

 * Reported (Current Year) = New cases reported

 * Chargesheeted = Cases where chargesheets were filed

 * Pending (End Year) = Cases pending at year-end

Cluster 0: Baseline Crime Activity Region

 * Moderate pending cases from previous year

 * Moderate reported cases during the year

 * Balanced level of chargesheet filings

 * Similar pending cases at end of year

Interpretation:
Represents typical districts with standard crime load and case processing rates.

Cluster 1: High Crime Reporting but Slow Case Disposal

 * High new cases reported

 * Lower chargesheet submission relative to reported cases

 * Large pending cases at year end

Interpretation:
Districts with high crime rates and slow investigation/clearance ‚Äî possible police resource strain.

Cluster 2: Strong Law Enforcement Efficiency

 * Moderate crime reporting

 * High chargesheets submitted

 * Lower pending cases at end of year

Interpretation:
Regions showing efficient investigation and higher case disposal, strong policing/legal processing.

Cluster 3: Crime Hotspots

 * Very high previous-year pendencies

 * High current year reporting

 * High chargesheet figures

 * Huge pending cases at end

Interpretation:
Major crime-dense districts or urban hubs ‚Äî high activity + backlog despite high case processing.

Analysis of Noise Points (cluster = -1)

Noise points represent:

 * Outlier months/districts with extreme spikes or dips in crime

 * Unusual reporting behavior

 * Rare events like sudden crackdown drives / mass filings

 * Possible data entry anomalies

 * DBSCAN flagged them because they don't fit normal density patterns.
"""

noise_points = df[df['cluster'] == -1]
print("\nCharacteristics of Noise Points (Cluster -1):")
st.dataframe(noise_points.describe())

"""Comparing Noise Points vs Overall Crime Dataset:

When comparing descriptive statistics of the noise points (-1 cluster) with the overall dataset:

The maximum values for crime indicators such as:

 * Pending (Prev Year)

 * Reported (Current Year)

 * Chargesheeted Cases

 * Pending (End Year)
are extremely high relative to the dataset‚Äôs overall mean and upper quartile (75th percentile).

This clearly shows that noise points correspond to high-severity crime spikes or extreme backlog values.

DBSCAN correctly identified these unusual data patterns as non-clusterable outliers, meaning they do not follow the same density pattern as regular districts/months.

These noise entries likely represent:

 * Extremely crime-heavy districts

 * Exceptional reporting months (crackdowns, special drives)

 * Sudden increases in pending cases or filings

 * Records affected by administrative or data reporting anomalies

‚úÖ DBSCAN Clustering Results ‚Äî Crime Dataset

Cluster Summary

DBSCAN identified 4 meaningful clusters (excluding noise)

A noticeable number of records labeled -1 (noise) represent outliers

These outliers correspond to unusually high crime or pending case values

 * Cluster-wise Interpretation
Cluster 0 ‚Äî Majority / Normal Crime Pattern

Contains most records

Represents typical crime levels across districts/months

Balanced case reporting and investigation behavior

Cluster 1 ‚Äî High Pending Case / Backlog Cluster

Smaller cluster

Very high pending cases

Indicates backlog and investigation pressure

Possible lack of resources or delayed investigation system

Cluster 2 ‚Äî High Efficiency / Case Disposal Cluster

Few data points

High chargesheeted & disposal rates

Represents districts with efficient investigation and faster legal processing

Cluster 3 ‚Äî Crime Hotspot / High Activity Cluster

Small cluster

High values across reported, pending, and ongoing cases

Represents major high-crime zones

Needs special monitoring, resource deployment, and preventive strategies

 * Noise / Outliers (Cluster -1)

Represent extreme crime statistics

Highly unusual spikes in crime or pending cases

DBSCAN correctly isolated them as anomalies

Useful for investigation focus or auditing administrative reasons

 * Final Insight

Clustering allowed us to segment regions based on crime burden & investigation efficiency

Helps authorities to:

Identify high-crime hotspots

Recognize high-efficiency districts

Track backlog-affected regions

Detect anomalous months/districts for deeper review

Key Differences in Results:

 * K-Means clustered entire states, while DBSCAN clustered individual monthly crime records.

 * K-Means created 3 fixed clusters (because we set k=3), but DBSCAN automatically found 4 clusters based on data density.

 * K-Means assigned every record to a cluster, whereas DBSCAN marked some records as noise (-1) ‚Äî meaning unusual or extreme crime cases.

 * K-Means gives state-level crime trends; DBSCAN gives monthly & district-level crime patterns.

 * K-Means cannot naturally handle outliers, but DBSCAN identifies and separates crime spikes/outliers.

 * K-Means shows broad crime behavior across states, while DBSCAN detects abnormal crime months and hotspots.

TASK 10: Random forest:
"""

from sklearn.ensemble import RandomForestClassifier

# Train model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict
y_pred_rf = rf_model.predict(X_test)

# Evaluation
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

"""* It uses multiple decision trees

 * Reduces overfitting

 * Gives higher accuracy

 * Works well even with small/medium datasets

 I applied a Random Forest Classifier which builds multiple decision trees and combines their results.
This improves accuracy and reduces overfitting compared to a single decision tree.

TASK 9: Outlier Detection:
 * Outlier detection helps identify unusual high or low crime values that do not follow the normal pattern.
We use the Interquartile Range (IQR) method:

 * IQR = Q3 ‚àí Q1

 * Lower bound = Q1 ‚àí 1.5 √ó IQR

 * Upper bound = Q3 + 1.5 √ó IQR

 * Values outside this range = Outliers
"""

# Outlier Detection using IQR Method

crime_values = df['Cases Reported during the year']

Q1 = crime_values.quantile(0.25)
Q3 = crime_values.quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(crime_values < lower_bound) | (crime_values > upper_bound)]

print("Lower Bound:", lower_bound)
print("Upper Bound:", upper_bound)
print("\nOutliers detected:")
print(outliers)

plt.figure(figsize=(6,4))
plt.boxplot(df['Cases Reported during the year'])
plt.title("Boxplot - Crime Outliers Detection")
plt.ylabel("Crime Cases Reported")
plt.grid(True)
plt.show()

"""Used the IQR method to detect outliers in crime cases.
Any value outside the range Q1 ‚àí 1.5√óIQR and Q3 + 1.5√óIQR is considered an outlier.
Outliers indicate abnormal spikes in crime numbers which may represent unusual events or reporting anomalies.

TASK 11: Dashboard Creation:
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit plotly-express

"""Now that the necessary libraries are installed, I'll regenerate the code to import them."""

import streamlit as st
import pandas as pd
import plotly.express as px

import pandas as pd
import numpy as np

# Load dataset
file_path = "NCRB_CII-2019_Table_17A.1 (1).csv"
df = pd.read_csv(file_path, low_memory=False)

# Remove duplicate rows
df = df.drop_duplicates()

# Fill missing numeric values with 0
num_cols = df.select_dtypes(include=[np.number]).columns
df[num_cols] = df[num_cols].fillna(0)

# Fill missing string values with "Unknown"
str_cols = df.select_dtypes(include=["object"]).columns
df[str_cols] = df[str_cols].fillna("Unknown")

# Fix numeric columns that contain commas (e.g., "1,234")
for col in df.columns:
    if df[col].dtype == "object":
        df[col] = df[col].str.replace(',', '', regex=True)
        # Convert to numeric if possible
        df[col] = pd.to_numeric(df[col], errors='ignore')

# Add the 'cluster' column if it doesn't exist, filling with a default or based on logic
# This is a placeholder based on previous steps, you might need to run the clustering code
# to get actual cluster labels if they are needed for the dashboard.
if 'cluster' not in df.columns:
    df['cluster'] = 0 # Default cluster label or implement logic to assign

# Create crime_clustering_features if not already created
numerical_quantity_cols = [
    'Cases Pending Investigation from Previous Year',
    'Cases Reported during the year',
    'Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)',
    'Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)'
]
crime_clustering_features = df[numerical_quantity_cols].copy()
# Assuming scaling and clustering will be done before running the streamlit app,
# or handle it within the app if necessary. For now, just creating the dataframe.

import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN

# Load dataset
file_path = "NCRB_CII-2019_Table_17A.1 (1).csv"
df = pd.read_csv(file_path, low_memory=False)

# Remove duplicate rows
df = df.drop_duplicates()

# Fill missing numeric values with 0
num_cols = df.select_dtypes(include=[np.number]).columns
df[num_cols] = df[num_cols].fillna(0)

# Fill missing string values with "Unknown"
str_cols = df.select_dtypes(include=["object"]).columns
df[str_cols] = df[str_cols].fillna("Unknown")

# Fix numeric columns that contain commas (e.g., "1,234")
for col in df.columns:
    if df[col].dtype == "object":
        df[col] = df[col].str.replace(',', '', regex=True)
        # Convert to numeric if possible
        df[col] = pd.to_numeric(df[col], errors='ignore')

# Rename columns with special characters for easier plotting
df = df.rename(columns={
    'Cases Pending Investigation from Previous Year': 'Cases Pending Investigation (Prev Year)',
    'Total Cases for Investigation (Col.3+ Col.4+ Col.5)': 'Total Cases for Investigation',
    'Final Report - Total (Col.10+ Col.11+ Col.12+ Col.13+ Col.14)': 'Final Report - Total',
    'Chargesheets submitted - Cases Chargesheeted Out of Cases from Previous Year': 'Chargesheets submitted - Cases Chargesheeted (Prev Year)',
    'Chargesheets submitted - Cases Chargesheeted Out of Cases during the Year': 'Chargesheets submitted - Cases Chargesheeted (Current Year)',
    'Chargesheets submitted - Cases Chargesheeted (Col.16+ Col.17)': 'Chargesheets submitted - Cases Chargesheeted',
    'Total Cases Disposed Off by Police (Col.7+ Col.8+ Col.15+ Col.18)': 'Total Cases Disposed Off by Police',
    'Cases Pending Investigation at End of the Year (Col.6- Col.9- Col.19- Col.20)': 'Cases Pending Investigation at End of Year',
    'Charge-Sheeting Rate (Col.18/ Col.19) *100': 'Charge-Sheeting Rate (%)',
    'Pendency Percentage (Col.22/ Col.6) *100': 'Pendency Percentage (%)'
})

# Perform DBSCAN clustering and add cluster labels
numerical_quantity_cols = [
    'Cases Pending Investigation (Prev Year)',
    'Cases Reported during the year',
    'Chargesheets submitted - Cases Chargesheeted', # Corrected column name
    'Cases Pending Investigation at End of Year'
]
crime_clustering_features = df[numerical_quantity_cols].copy()
scaler = StandardScaler()
scaled_crime_clustering_features = scaler.fit_transform(crime_clustering_features)
dbscan = DBSCAN(eps=0.5, min_samples=5) # You might need to tune these parameters
crime_cluster_labels = dbscan.fit_predict(scaled_crime_clustering_features) # Fit and predict to get labels
df['cluster'] = crime_cluster_labels

st.title("üìä Crime Analysis Dashboard")
st.write("Dataset Loaded and Processed Successfully ‚úÖ")

st.sidebar.header("Filters")
crime_columns = df.columns[1:]   # Excluding first column (ID or category)

selected_crime = st.sidebar.selectbox("Select Crime Column", crime_columns)

st.subheader("üìÇ Dataset Preview")
st.dataframe(df.head())

st.subheader("üìà Summary Statistics")
st.write(df.describe())

st.subheader("üîó Correlation Heatmap")

# Select only numeric columns for correlation calculation
numeric_df = df.select_dtypes(include=[np.number])
corr = numeric_df.corr()

fig_corr = px.imshow(
    corr,
    text_auto=True,
    title="Correlation Heatmap"
)

st.plotly_chart(fig_corr)

st.subheader(f"üìç Crime Distribution ‚Äî {selected_crime}")

fig_bar = px.bar(
    df,
    y=selected_crime,
    title=f"{selected_crime} ‚Äî Bar Chart"
)

st.plotly_chart(fig_bar)

import streamlit as st
import pandas as pd
import plotly.express as px
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN

st.subheader("üìé Crime Feature Comparison")

# Select only numeric columns for plotting
crime_columns_numeric_only = df.select_dtypes(include=[np.number]).columns.tolist()

x_axis = st.selectbox("Select X-axis", crime_columns_numeric_only)
y_axis = st.selectbox("Select Y-axis", crime_columns_numeric_only)

# Re-run DBSCAN clustering and add cluster labels to the main dataframe
numerical_quantity_cols = [
    'Cases Pending Investigation (Prev Year)',
    'Cases Reported during the year',
    'Chargesheets submitted - Cases Chargesheeted',
    'Cases Pending Investigation at End of Year'
]
crime_clustering_features_for_scaling = df[numerical_quantity_cols].copy()
scaler = StandardScaler()
scaled_crime_clustering_features = scaler.fit_transform(crime_clustering_features_for_scaling)
# You might need to tune these parameters
dbscan = DBSCAN(eps=0.5, min_samples=5)
df['cluster'] = dbscan.fit_predict(scaled_crime_clustering_features)


# Ensure both selected columns are numeric before plotting with trendline
if x_axis in crime_columns_numeric_only and y_axis in crime_columns_numeric_only:
    # Check if 'cluster' column is available in df for coloring
    if 'cluster' in df.columns:
        # Convert cluster labels to string for proper coloring by plotly express
        df['cluster'] = df['cluster'].astype(str)
        fig_scatter = px.scatter(df, x=x_axis, y=y_axis, color='cluster') # Use df for plotting and color='cluster'
    else:
        fig_scatter = px.scatter(df, x=x_axis, y=y_axis) # Plot without color if cluster is not available

    st.plotly_chart(fig_scatter)

else:
    st.warning("Please select two numeric columns for the scatter plot.")

st.subheader("‚¨á Download Processed Dataset")

csv_file = df.to_csv(index=False)

st.download_button(
    label="Download CSV",
    data=csv_file,
    file_name="processed_crime_data.csv",
    mime="text/csv"
)

"""TASK 11: Decision Tree:"""

import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

# Ensure 'Crime_Level' exists (copied from Feature Engineering step)
median_crime = df['Cases Reported during the year'].median()
df['Crime_Level'] = (df['Cases Reported during the year'] > median_crime).astype(int)

# Prepare data for Decision Tree
# Let's use some relevant features from your existing 'df' for demonstration
# For simplicity, let's use 'Chargesheets submitted - Cases Chargesheeted'
# and 'Cases Pending Investigation at End of Year' to predict 'Crime_Level'

X_dt = df[['Chargesheets submitted - Cases Chargesheeted', 'Cases Pending Investigation at End of Year']]
y_dt = df['Crime_Level']

# Split data (using the same split ratio for consistency, though not strictly necessary for visualization)
X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(X_dt, y_dt, test_size=0.2, random_state=42)

# Train a Decision Tree Classifier
dt_model = DecisionTreeClassifier(max_depth=3, random_state=42) # Limiting depth for better visualization
dt_model.fit(X_train_dt, y_train_dt)

# Visualize the Decision Tree
plt.figure(figsize=(20, 10))
plot_tree(dt_model, filled=True, feature_names=X_dt.columns, class_names=['Low Crime', 'High Crime'], rounded=True)
plt.title('Decision Tree for Crime Level Prediction')
plt.show()
plt.close() # Close the figure to manage memory